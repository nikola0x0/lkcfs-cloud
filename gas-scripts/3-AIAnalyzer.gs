/**
 * AI Analyzer - Uses Google Gemini API as primary, OpenRouter as fallback
 */

// OpenRouter fallback model (used when Gemini fails)
// Gemini Free tier = 1,500 requests/day (PRIMARY - 30x more generous!)
// OpenRouter Free tier = 50 requests/day (FALLBACK - saves for emergencies)
const OPENROUTER_FALLBACK_MODELS = [
  "tngtech/deepseek-r1t2-chimera:free"            // Chimera (good reasoning, emergency backup)
];

/**
 * Analyze confessions using AI with Gemini-primary architecture
 * Priority: Gemini (1,500/day) â†’ OpenRouter (50/day)
 */
function analyzeWithAI(confessions) {
  if (!confessions || confessions.length === 0) {
    Logger.log('No confessions to analyze');
    return [];
  }

  const confessionText = prepareTextForAI(confessions);

  // Try Gemini FIRST (primary API with 1,500 requests/day)
  if (CONFIG.USE_GEMINI_FALLBACK) {
    Logger.log('ðŸš€ Using Google Gemini API (primary - 1,500 requests/day)');
    try {
      const result = analyzeWithGemini(confessionText);
      if (result) {
        Logger.log('âœ… Gemini analysis successful');
        return result;
      }
    } catch (geminiError) {
      Logger.log('âš ï¸ Gemini failed, falling back to OpenRouter: ' + geminiError.toString());
    }
  }

  // Gemini failed or disabled, try OpenRouter models as fallback
  Logger.log('ðŸ”„ Falling back to OpenRouter (50 requests/day)');
  const modelsToTry = [CONFIG.OPENROUTER_MODEL, ...OPENROUTER_FALLBACK_MODELS.filter(m => m !== CONFIG.OPENROUTER_MODEL)];

  for (let i = 0; i < modelsToTry.length; i++) {
    const model = modelsToTry[i];
    try {
      Logger.log(`Trying OpenRouter model ${i + 1}/${modelsToTry.length}: ${model}`);
      const result = tryAnalyzeWithModel(confessionText, model);
      if (result) {
        Logger.log(`âœ… Success with OpenRouter model: ${model}`);
        return result;
      }
    } catch (error) {
      const errorMsg = error.toString();

      // Check if it's a rate limit error (429)
      if (errorMsg.includes('429') || errorMsg.includes('Rate limit')) {
        Logger.log(`âš ï¸ Model ${model} hit rate limit, trying next model...`);
        continue;
      }

      // For other errors (404, etc), log and try next model
      Logger.log(`âŒ Model ${model} failed: ${errorMsg}`);
      if (i === modelsToTry.length - 1) {
        // Last model, throw comprehensive error
        throw new Error('All AI services failed:\n' +
          '1. Google Gemini exhausted or failed (1,500/day limit)\n' +
          '2. OpenRouter models exhausted or rate limited (50/day)\n' +
          'Solutions:\n' +
          '- Wait for Gemini reset (usually recovers quickly)\n' +
          '- Wait for OpenRouter reset at midnight UTC\n' +
          '- Check API keys at https://makersuite.google.com/app/apikey\n' +
          'Original error: ' + errorMsg);
      }
      continue;
    }
  }

  // This should never be reached due to the throw above, but just in case
  throw new Error('All AI services exhausted. Please check API keys and rate limits.');
}

/**
 * Try to analyze with a specific model
 */
function tryAnalyzeWithModel(confessionText, model) {
  try {
    const systemPrompt = `You are analyzing Vietnamese high school student confessions. Extract topics that show what's TRENDING at school right now - specific enough to be interesting, general enough to group similar confessions.

Return ONLY a valid JSON array (no markdown, no explanations) with this exact format:
[
  {
    "topic": "Topic name in Vietnamese (4-8 words)",
    "count": 5,
    "sentiment": 0.3
  }
]

CRITICAL Requirements:
- Return 5-12 topics maximum
- Only include topics mentioned by at least ${CONFIG.MIN_TOPIC_COUNT} confessions
- Balance SPECIFICITY with GROUPING:

  âŒ TOO SPECIFIC: "Crush báº¡n THTV B3, XuÃ¢n Nghi B4, Nháº­t Long A10" â†’ Don't list individual names
  âŒ TOO GENERAL: "Há»i thÃ´ng tin báº¡n há»c / Crush" â†’ Too boring!
  âœ… JUST RIGHT: "Xin info báº¡n ná»¯/nam xinh/Ä‘áº¹p trong trÆ°á»ng" â†’ Tells what's happening without naming people

  âŒ TOO SPECIFIC: "Review tháº§y VÄƒn Trung dáº¡y khÃ³ hiá»ƒu"
  âŒ TOO GENERAL: "GÃ³p Ã½ giÃ¡o viÃªn"
  âœ… JUST RIGHT: "Tháº§y VÄƒn dáº¡y khÃ³ hiá»ƒu muá»‘n Ä‘á»•i lá»›p" â†’ Mentions subject/issue, groups similar teachers

  âŒ TOO SPECIFIC: "Máº¥t vÃ­, son, tiá»n dÆ°á»›i gá»‘c cÃ¢y"
  âŒ TOO GENERAL: "Máº¥t Ä‘á»“ á»Ÿ trÆ°á»ng"
  âœ… JUST RIGHT: "Máº¥t Ä‘á»“ á»Ÿ cÄƒn tin giá»¯a giá» ra chÆ¡i" â†’ Tells WHERE and WHEN

  âŒ TOO SPECIFIC: "Xin tÃ i liá»‡u ToÃ¡n tháº§y Äá»©c 12, lá»›p 11 cÅ©"
  âŒ TOO GENERAL: "Xin tÃ i liá»‡u há»c táº­p"
  âœ… JUST RIGHT: "Xin tÃ i liá»‡u Ã´n ToÃ¡n cuá»‘i ká»³ lá»›p 12" â†’ Tells WHAT subject and WHY

  âœ… MORE GOOD EXAMPLES:
  - "Äá»“ng phá»¥c thá»ƒ dá»¥c cháº­m phÃ¡t ná»­a ká»³"
  - "Láº­p Ä‘á»™i bÃ³ng chuyá»n ná»¯ thiáº¿u ngÆ°á»i"
  - "Lá»›p C12 friendly vá»›i cáº£ trÆ°á»ng"
  - "Kiá»ƒm duyá»‡t confes lá»ng quÃ¡ nhiá»u spam"
  - "ToÃ¡n cuá»‘i ká»³ khÃ³ sá»£ rá»›t mÃ´n"

RULES:
- Group confessions asking about DIFFERENT people â†’ "Xin info báº¡n Ä‘áº¹p/xinh trong trÆ°á»ng"
- Group confessions about DIFFERENT teachers of SAME subject â†’ "Tháº§y VÄƒn dáº¡y khÃ³ hiá»ƒu"
- Group confessions about SIMILAR items lost â†’ "Máº¥t Ä‘á»“ á»Ÿ cÄƒn tin"
- Keep SPECIFIC DETAILS that make it interesting: subjects, locations, time, emotions, specific events
- **IMPORTANT**: If 3+ confessions mention THE SAME person/class/teacher â†’ KEEP IT SPECIFIC!
  âœ… "Tháº§y VÄƒn Trung dáº¡y VÄƒn khÃ³ hiá»ƒu" (if 5 confessions about him specifically)
  âœ… "Lá»›p 12A1 á»“n Ã o lÃ m phiá»n lá»›p khÃ¡c" (if many confessions about this class)
  âœ… "Báº¡n Minh 12C há»c giá»i giÃºp báº¡n" (if many confessions praise this person)
- Remove names ONLY if confessions are about DIFFERENT people
- Make students think "oh yeah that's happening right now!"
- Count represents how many confessions match this topic
- Sentiment: -1.0 (very negative) to +1.0 (very positive), averaged across grouped confessions
- Return ONLY the JSON array, no other text`;

  const userPrompt = `Extract topics from these Vietnamese student confessions:

${confessionText}

Remember: Return ONLY the JSON array, no markdown code blocks, no explanations.`;

  const requestBody = {
    model: model,  // Use the model passed as parameter
    messages: [
      {
        role: "system",
        content: systemPrompt
      },
      {
        role: "user",
        content: userPrompt
      }
    ],
    temperature: 0.3,
    max_tokens: 4096
  };

    const options = {
      method: 'post',
      contentType: 'application/json',
      headers: {
        'Authorization': 'Bearer ' + CONFIG.OPENROUTER_API_KEY,
        'HTTP-Referer': 'https://lkcfs.edu.vn',
        'X-Title': 'LK Confessions Analyzer'
      },
      payload: JSON.stringify(requestBody),
      muteHttpExceptions: true
    };

    Logger.log('Sending request to OpenRouter...');
    const response = UrlFetchApp.fetch(CONFIG.OPENROUTER_API_URL, options);
    const responseCode = response.getResponseCode();

    if (responseCode !== 200) {
      throw new Error(`API Error ${responseCode}: ${response.getContentText()}`);
    }

    const result = JSON.parse(response.getContentText());
    Logger.log('Full API Response: ' + JSON.stringify(result));

    if (!result.choices || result.choices.length === 0) {
      throw new Error('No response from OpenRouter API');
    }

    const aiResponse = result.choices[0].message.content;
    Logger.log('AI Response: ' + aiResponse);

    // Clean response - remove markdown code blocks if present
    let cleanedResponse = aiResponse.trim();
    cleanedResponse = cleanedResponse.replace(/```json\s*/g, '');
    cleanedResponse = cleanedResponse.replace(/```\s*/g, '');
    cleanedResponse = cleanedResponse.trim();

    // Extract JSON array
    const jsonMatch = cleanedResponse.match(/\[[\s\S]*\]/);
    if (!jsonMatch) {
      throw new Error('Could not parse JSON from AI response: ' + cleanedResponse);
    }

    const topics = JSON.parse(jsonMatch[0]);

    // Filter by minimum count
    const filteredTopics = topics.filter(t => t.count >= CONFIG.MIN_TOPIC_COUNT);

    Logger.log(`Extracted ${filteredTopics.length} topics`);
    return filteredTopics;

  } catch (error) {
    Logger.log('Error in AI analysis: ' + error.toString());
    throw error;
  }
}

/**
 * Analyze with Google Gemini API (Ultimate Fallback)
 * Called when all OpenRouter models fail or hit rate limits
 */
function analyzeWithGemini(confessionText) {
  if (!CONFIG.USE_GEMINI_FALLBACK) {
    throw new Error('Gemini fallback is disabled');
  }

  if (CONFIG.GEMINI_API_KEY === 'YOUR_GEMINI_API_KEY_HERE') {
    throw new Error('Gemini API key not configured. Get one at https://makersuite.google.com/app/apikey');
  }

  Logger.log(`ðŸš¨ Trying ultimate fallback: Google Gemini API (${CONFIG.GEMINI_MODEL})`);

  const prompt = `You are analyzing Vietnamese high school student confessions. Extract topics that show what's TRENDING at school right now - specific enough to be interesting, general enough to group similar confessions.

CRITICAL Requirements:
- Return 5-12 topics maximum
- Only include topics mentioned by at least ${CONFIG.MIN_TOPIC_COUNT} confessions
- Balance SPECIFICITY with GROUPING:

  âŒ TOO SPECIFIC: "Crush báº¡n THTV B3, XuÃ¢n Nghi B4, Nháº­t Long A10" â†’ Don't list individual names
  âŒ TOO GENERAL: "Há»i thÃ´ng tin báº¡n há»c / Crush" â†’ Too boring!
  âœ… JUST RIGHT: "Xin info báº¡n ná»¯/nam xinh/Ä‘áº¹p trong trÆ°á»ng" â†’ Tells what's happening without naming people

  âŒ TOO SPECIFIC: "Review tháº§y VÄƒn Trung dáº¡y khÃ³ hiá»ƒu"
  âŒ TOO GENERAL: "GÃ³p Ã½ giÃ¡o viÃªn"
  âœ… JUST RIGHT: "Tháº§y VÄƒn dáº¡y khÃ³ hiá»ƒu muá»‘n Ä‘á»•i lá»›p" â†’ Mentions subject/issue, groups similar teachers

  âŒ TOO SPECIFIC: "Máº¥t vÃ­, son, tiá»n dÆ°á»›i gá»‘c cÃ¢y"
  âŒ TOO GENERAL: "Máº¥t Ä‘á»“ á»Ÿ trÆ°á»ng"
  âœ… JUST RIGHT: "Máº¥t Ä‘á»“ á»Ÿ cÄƒn tin giá»¯a giá» ra chÆ¡i" â†’ Tells WHERE and WHEN

  âŒ TOO SPECIFIC: "Xin tÃ i liá»‡u ToÃ¡n tháº§y Äá»©c 12, lá»›p 11 cÅ©"
  âŒ TOO GENERAL: "Xin tÃ i liá»‡u há»c táº­p"
  âœ… JUST RIGHT: "Xin tÃ i liá»‡u Ã´n ToÃ¡n cuá»‘i ká»³ lá»›p 12" â†’ Tells WHAT subject and WHY

  âœ… MORE GOOD EXAMPLES:
  - "Äá»“ng phá»¥c thá»ƒ dá»¥c cháº­m phÃ¡t ná»­a ká»³"
  - "Láº­p Ä‘á»™i bÃ³ng chuyá»n ná»¯ thiáº¿u ngÆ°á»i"
  - "Lá»›p C12 friendly vá»›i cáº£ trÆ°á»ng"
  - "Kiá»ƒm duyá»‡t confes lá»ng quÃ¡ nhiá»u spam"
  - "ToÃ¡n cuá»‘i ká»³ khÃ³ sá»£ rá»›t mÃ´n"

RULES:
- Group confessions asking about DIFFERENT people â†’ "Xin info báº¡n Ä‘áº¹p/xinh trong trÆ°á»ng"
- Group confessions about DIFFERENT teachers of SAME subject â†’ "Tháº§y VÄƒn dáº¡y khÃ³ hiá»ƒu"
- Group confessions about SIMILAR items lost â†’ "Máº¥t Ä‘á»“ á»Ÿ cÄƒn tin"
- Keep SPECIFIC DETAILS that make it interesting: subjects, locations, time, emotions, specific events
- **IMPORTANT**: If 3+ confessions mention THE SAME person/class/teacher â†’ KEEP IT SPECIFIC!
  âœ… "Tháº§y VÄƒn Trung dáº¡y VÄƒn khÃ³ hiá»ƒu" (if 5 confessions about him specifically)
  âœ… "Lá»›p 12A1 á»“n Ã o lÃ m phiá»n lá»›p khÃ¡c" (if many confessions about this class)
  âœ… "Báº¡n Minh 12C há»c giá»i giÃºp báº¡n" (if many confessions praise this person)
- Remove names ONLY if confessions are about DIFFERENT people
- Make students think "oh yeah that's happening right now!"
- Count represents how many confessions match this topic
- Sentiment: -1.0 (very negative) to +1.0 (very positive), averaged across grouped confessions

Extract topics from these Vietnamese student confessions:

${confessionText}`;

  try {
    const requestBody = {
      contents: [{
        parts: [{
          text: prompt
        }]
      }],
      generationConfig: {
        temperature: 0.3,
        responseMimeType: "application/json",
        responseSchema: {
          type: "array",
          items: {
            type: "object",
            properties: {
              topic: {
                type: "string",
                description: "Topic name in Vietnamese"
              },
              count: {
                type: "integer",
                description: "Number of confessions about this topic"
              },
              sentiment: {
                type: "number",
                description: "Average sentiment from -1.0 to 1.0"
              }
            },
            required: ["topic", "count", "sentiment"]
          }
        },
        thinkingConfig: {
          thinkingBudget: CONFIG.GEMINI_THINKING_BUDGET,  // Configurable thinking budget
          includeThoughts: true  // Include thought summaries for debugging
        }
      }
    };

    const geminiEndpoint = CONFIG.GEMINI_API_URL + CONFIG.GEMINI_MODEL + ':generateContent?key=' + CONFIG.GEMINI_API_KEY;

    const options = {
      method: 'post',
      contentType: 'application/json',
      payload: JSON.stringify(requestBody),
      muteHttpExceptions: true
    };

    Logger.log('Gemini Endpoint: ' + geminiEndpoint);
    const response = UrlFetchApp.fetch(geminiEndpoint, options);
    const responseCode = response.getResponseCode();

    if (responseCode !== 200) {
      throw new Error(`Gemini API Error ${responseCode}: ${response.getContentText()}`);
    }

    const result = JSON.parse(response.getContentText());
    Logger.log('Full Gemini Response: ' + JSON.stringify(result));

    // Check response structure
    if (!result.candidates || result.candidates.length === 0) {
      throw new Error('No candidates in Gemini response: ' + JSON.stringify(result));
    }

    if (!result.candidates[0].content) {
      throw new Error('No content in Gemini candidate: ' + JSON.stringify(result.candidates[0]));
    }

    if (!result.candidates[0].content.parts || result.candidates[0].content.parts.length === 0) {
      throw new Error('No parts in Gemini content: ' + JSON.stringify(result.candidates[0].content));
    }

    // Gemini 2.5 with thinking returns multiple parts: thoughts + answer
    const parts = result.candidates[0].content.parts;
    let thoughtSummary = '';
    let jsonResponse = '';

    // Separate thoughts from the actual JSON response
    for (const part of parts) {
      if (part.thought) {
        thoughtSummary += part.text;
      } else if (part.text) {
        jsonResponse = part.text;
      }
    }

    // Log the thinking process for debugging
    if (thoughtSummary) {
      Logger.log('ðŸ§  Gemini Thinking Summary: ' + thoughtSummary.substring(0, 500) + '...');
    }

    Logger.log('Gemini JSON Response: ' + jsonResponse);

    // Parse the JSON response
    const topics = JSON.parse(jsonResponse);

    if (!Array.isArray(topics)) {
      throw new Error('Gemini response is not an array: ' + jsonResponse);
    }

    // Filter by minimum count
    const filteredTopics = topics.filter(t => t.count >= CONFIG.MIN_TOPIC_COUNT);

    // Log token usage
    if (result.usageMetadata) {
      Logger.log(`ðŸ“Š Token usage - Thinking: ${result.usageMetadata.thoughtsTokenCount || 0}, Output: ${result.usageMetadata.candidatesTokenCount}, Total: ${result.usageMetadata.totalTokenCount}`);
    }

    Logger.log(`âœ… Gemini succeeded! Extracted ${filteredTopics.length} topics`);
    return filteredTopics;

  } catch (error) {
    Logger.log('Gemini fallback failed: ' + error.toString());
    throw error;
  }
}

/**
 * Test AI analysis
 */
function testAIAnalysis() {
  Logger.log('=== Testing AI Analysis with OpenRouter ===');

  const confessions = getRecentConfessions();
  Logger.log(`Got ${confessions.length} confessions`);

  const topics = analyzeWithAI(confessions);
  Logger.log(`\nExtracted ${topics.length} topics:`);

  topics.forEach((topic, index) => {
    Logger.log(`${index + 1}. ${topic.topic} (Count: ${topic.count}, Sentiment: ${topic.sentiment})`);
  });

  return topics;
}
